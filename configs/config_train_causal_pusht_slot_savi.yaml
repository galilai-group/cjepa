#output_dir: ./outputs

defaults:
  - _self_
  - override hydra/hydra_logging: disabled
  - override hydra/job_logging: disabled

hydra:
  output_subdir: null

wandb:
  enable: true
  project: cjepa_predictor_only
  entity: heejeong_nam-brown-university


dataset_name: pusht_expert
embedding_dir: '/cs/data/people/hnam16/data/modified_extraction/pusht_expert_slots_pushtnoise_videosaur_lr1e-4_w03_step=100000.pkl'
cache_dir: "/cs/data/people/hnam16/.stable_worldmodel"
action_dir: '/cs/data/people/hnam16/data/modified_extraction/pusht_expert_action_meta.pkl'
proprio_dir: '/cs/data/people/hnam16/data/modified_extraction/pusht_expert_proprio_meta.pkl'
state_dir: '/cs/data/people/hnam16/data/modified_extraction/pusht_expert_state_meta.pkl'

output_model_name: causal_world_model
training_type: video  # options: video, wm

trainer:
  max_epochs: 100
  # strategy: ddp 
  devices: auto
  accelerator: gpu 
  precision: 16-mixed
  log_every_n_steps: 1

batch_size: 256
num_workers: 10

seed: 42

image_size: 64
patch_size: 16

n_steps: ${eval:'${dinowm.num_preds} + ${dinowm.history_size}'}
frameskip: 3

savi:
  weight: /cs/data/people/hnam16/savi_pretrained/ex101_20260108_023722_LR0.0001/savi/epoch/model_24.pth
  params: slotformer/base_slots/configs/savi_pusht_ind_params_mlp.py
  NUM_SLOTS: 4
  SLOT_DIM: 128


dinowm:
  history_size: 5
  num_preds: 3
  proprio_dim: 4
  proprio_embed_dim: 10
  action_dim: 2
  action_embed_dim: 10

# Masked Slot Predictor Configuration
num_masked_slots: 2  # M: number of slots to mask during training

predictor:
  depth: 6
  heads: 16
  mlp_dim: 2048
  dim_head: 64
  dropout: 0.1
  emb_dropout: 0.0

predictor_lr: 5e-4
proprio_encoder_lr: 5e-4
action_encoder_lr: 5e-4

dump_object: true

